{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Updated LangChain imports\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI  # Updated imports\n",
    "from langchain_community.vectorstores import FAISS \n",
    "# from langchain_community.vectorstores import Qdrant  # Updated import\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Qdrant client\n",
    "#from qdrant_client import QdrantClient\n",
    "#from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# RAGAS evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "\n",
    "# Tavily for web search\n",
    "from tavily import TavilyClient\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API Key:\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Document Ingestion and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, data_path: str = \"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data\"):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.chunk_size = 800  # tokens as per project spec\n",
    "        self.chunk_overlap = 100  # tokens as per project spec\n",
    "        \n",
    "    def load_documents(self) -> List[Document]:\n",
    "        \"\"\"Load all markdown documents from the data directory\"\"\"\n",
    "        loader = DirectoryLoader(\n",
    "            str(self.data_path),\n",
    "            glob=\"**/*.md\",\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={'encoding': 'utf-8'}\n",
    "        )\n",
    "        documents = loader.load()\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "        return documents\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents into chunks using the specified strategy\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vector Store Setup with Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, qdrant_url: str = \":memory:\"):\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.client = QdrantClient(qdrant_url)\n",
    "        self.embedding_dim = len(self.embeddings.embed_query(\"test\"))\n",
    "\n",
    "    def create_vectorstore(self, chunks: List[Document], collection_name: str = \"rfp_docs\", recreate: bool = True):\n",
    "        \"\"\"\n",
    "        Create (or recreate) a Qdrant vector store from document chunks.\n",
    "        If recreate=True, it will delete and recreate the collection if it already exists.\n",
    "        \"\"\"\n",
    "        existing_collections = [c.name for c in self.client.get_collections().collections]\n",
    "\n",
    "        # ‚úÖ Handle recreation cleanly\n",
    "        if collection_name in existing_collections:\n",
    "            if recreate:\n",
    "                self.client.delete_collection(collection_name)\n",
    "                print(f\"üßπ Deleted existing collection: {collection_name}\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è  Using existing collection: {collection_name}\")\n",
    "\n",
    "        # ‚úÖ Create fresh collection if needed\n",
    "        if recreate or collection_name not in existing_collections:\n",
    "            self.client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=self.embedding_dim,\n",
    "                    distance=Distance.COSINE\n",
    "                )\n",
    "            )\n",
    "            print(f\"‚úÖ Created collection: {collection_name}\")\n",
    "\n",
    "        # ‚úÖ Use explicit client-based Qdrant integration\n",
    "        try:\n",
    "            vectorstore = Qdrant(\n",
    "                client=self.client,\n",
    "                collection_name=collection_name,\n",
    "                embeddings=self.embeddings\n",
    "            )\n",
    "            vectorstore.add_documents(chunks)\n",
    "            print(f\"‚úÖ Added {len(chunks)} documents to vector store '{collection_name}'\")\n",
    "            return vectorstore\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to add documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def delete_collection(self, collection_name: str):\n",
    "        \"\"\"Manually delete a Qdrant collection\"\"\"\n",
    "        existing_collections = [c.name for c in self.client.get_collections().collections]\n",
    "        if collection_name in existing_collections:\n",
    "            self.client.delete_collection(collection_name)\n",
    "            print(f\"üßπ Deleted collection: {collection_name}\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è Collection '{collection_name}' not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RFP Agent with Advanced Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from tavily import TavilyClient\n",
    "\n",
    "class RFPAgent:\n",
    "    def __init__(self, vectorstore, tavily_client: TavilyClient = None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
    "        self.tavily_client = tavily_client or TavilyClient()\n",
    "\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        self.tools = self._create_tools()\n",
    "        self.agent = self._create_agent()\n",
    "\n",
    "    def _create_tools(self) -> List[Tool]:\n",
    "        def search_documentation(query: str) -> str:\n",
    "            try:\n",
    "                results = self.qa_chain({\"query\": query})\n",
    "                answer = results[\"result\"]\n",
    "                sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in results[\"source_documents\"]]\n",
    "                unique_sources = \", \".join(sorted(set(sources)))\n",
    "                return f\"Answer: {answer}\\n\\nSources: {unique_sources}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error searching documentation: {str(e)}\"\n",
    "\n",
    "        def search_web(query: str) -> str:\n",
    "            try:\n",
    "                response = self.tavily_client.search(query=query, max_results=3)\n",
    "                results = []\n",
    "                for result in response.get(\"results\", []):\n",
    "                    results.append(\n",
    "                        f\"Title: {result.get('title','N/A')}\\n\"\n",
    "                        f\"Content: {result.get('content','')}\\n\"\n",
    "                        f\"URL: {result.get('url','')}\"\n",
    "                    )\n",
    "                return \"\\n\\n\".join(results) if results else \"No results found.\"\n",
    "            except Exception as e:\n",
    "                return f\"Error searching web: {str(e)}\"\n",
    "\n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"search_documentation\",\n",
    "                description=\"Search internal company documentation for RFP responses and technical specifications\",\n",
    "                func=search_documentation\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_web\",\n",
    "                description=\"Search the web for supplementary information, competitor analysis, or current market trends\",\n",
    "                func=search_web\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _create_agent(self):\n",
    "        return initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def respond_to_rfp(self, question: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            response = self.agent.invoke({\"input\": question})\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": response[\"output\"],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": f\"Error generating response: {str(e)}\",\n",
    "                \"status\": \"error\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n",
      "Created 27 chunks from 3 documents\n",
      "‚úÖ Created collection: rfp_docs\n",
      "‚úÖ Added 27 documents to vector store 'rfp_docs'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/bgf_wkp116b2ywt2295ppgv80000gn/T/ipykernel_854/1523123900.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to check our internal documentation for information on our product's compliance standards.\n",
      "Action: search_documentation\n",
      "Action Input: \"product compliance standards\"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tk/bgf_wkp116b2ywt2295ppgv80000gn/T/ipykernel_854/1523123900.py:26: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = self.qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: The platform meets the following compliance standards:\n",
      "- SOC 2 Type II: This pertains to security, availability, and confidentiality controls.\n",
      "- ISO 27001: This is related to the information security management system.\n",
      "- GDPR: This is for data privacy and protection compliance.\n",
      "- HIPAA: This is for healthcare data protection and is an optional add-on.\n",
      "- PCI DSS: This is for payment card industry compliance and is also an optional add-on.\n",
      "\n",
      "Sources: /Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data/sample_faq.md, /Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data/sample_product_specs.md, /Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data/sample_rfp_responses.md\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Our product meets the following compliance standards: SOC 2 Type II, ISO 27001, GDPR. Additionally, we offer HIPAA and PCI DSS as optional add-ons.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Our product meets the following compliance standards: SOC 2 Type II, ISO 27001, GDPR. Additionally, we offer HIPAA and PCI DSS as optional add-ons.\n"
     ]
    }
   ],
   "source": [
    "# 1. Import your document processor and vector store manager\n",
    "processor = DocumentProcessor(\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data\")\n",
    "\n",
    "# 2. Load the markdown documents\n",
    "documents = processor.load_documents()\n",
    "\n",
    "# 3. Chunk/split the documents into smaller pieces\n",
    "chunks = processor.chunk_documents(documents)\n",
    "\n",
    "# 4. Create the vector store (Qdrant)\n",
    "vector_manager = VectorStoreManager()\n",
    "vectorstore = vector_manager.create_vectorstore(chunks, collection_name=\"rfp_docs\")\n",
    "\n",
    "# 5. Initialize your RFP agent\n",
    "agent = RFPAgent(vectorstore=vectorstore)\n",
    "\n",
    "# 6. Ask a question\n",
    "response = agent.respond_to_rfp(\"What are the compliance standards for our product?\")\n",
    "print(response[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFPAgent:\n",
    "    def __init__(self, vectorstore, tavily_client=None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = OpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
    "        self.tavily_client = tavily_client or TavilyClient()\n",
    "        \n",
    "        # Create retrieval QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        # Create tools for the agent\n",
    "        self.tools = self._create_tools()\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = self._create_agent()\n",
    "    \n",
    "    def _create_tools(self) -> List[Tool]:\n",
    "        \"\"\"Create tools for the RFP agent\"\"\"\n",
    "        \n",
    "        def search_documentation(query: str) -> str:\n",
    "            \"\"\"Search internal documentation for RFP responses\"\"\"\n",
    "            try:\n",
    "                results = self.qa_chain({\"query\": query})\n",
    "                sources = [doc.metadata.get('source', 'Unknown') for doc in results['source_documents']]\n",
    "                return f\"Answer: {results['result']}\\n\\nSources: {', '.join(set(sources))}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error searching documentation: {str(e)}\"\n",
    "        \n",
    "        def search_web(query: str) -> str:\n",
    "            \"\"\"Search web for supplementary information\"\"\"\n",
    "            try:\n",
    "                response = self.tavily_client.search(query=query, max_results=3)\n",
    "                results = []\n",
    "                for result in response['results']:\n",
    "                    results.append(f\"Title: {result['title']}\\nContent: {result['content']}\\nURL: {result['url']}\")\n",
    "                return \"\\n\\n\".join(results)\n",
    "            except Exception as e:\n",
    "                return f\"Error searching web: {str(e)}\"\n",
    "        \n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"search_documentation\",\n",
    "                description=\"Search internal company documentation for RFP responses and technical specifications\",\n",
    "                func=search_documentation\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_web\",\n",
    "                description=\"Search the web for supplementary information, competitor analysis, or current market trends\",\n",
    "                func=search_web\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def _create_agent(self) -> AgentExecutor:\n",
    "        \"\"\"Create the RFP agent with tools\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=\"\"\"You are an expert Solutions Engineer helping respond to customer RFPs and technical questions.\n",
    "\n",
    "You have access to:\n",
    "1. Internal company documentation (product specs, RFP responses, FAQs)\n",
    "2. Web search for supplementary information\n",
    "\n",
    "Guidelines:\n",
    "- Always prioritize internal documentation for product-specific questions\n",
    "- Use web search for market trends, competitor analysis, or general industry information\n",
    "- Provide comprehensive, accurate responses with proper citations\n",
    "- Structure responses clearly with bullet points when appropriate\n",
    "- Include relevant technical specifications and performance metrics\n",
    "- Reference specific compliance standards and certifications when applicable\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Use the available tools to gather information and provide a comprehensive response.\"\"\",\n",
    "            input_variables=[\"input\"]\n",
    "        )\n",
    "        \n",
    "        agent = create_openai_tools_agent(\n",
    "            llm=self.llm,\n",
    "            tools=self.tools,\n",
    "            prompt=prompt\n",
    "        )\n",
    "        \n",
    "        return AgentExecutor(agent=agent, tools=self.tools, verbose=True)\n",
    "    \n",
    "    def respond_to_rfp(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate RFP response using the agent\"\"\"\n",
    "        try:\n",
    "            response = self.agent.invoke({\"input\": question})\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": response[\"output\"],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": f\"Error generating response: {str(e)}\",\n",
    "                \"status\": \"error\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from tavily import TavilyClient\n",
    "\n",
    "class RFPAgent:\n",
    "    def __init__(self, vectorstore, tavily_client: TavilyClient = None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
    "        self.tavily_client = tavily_client or TavilyClient()\n",
    "\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        self.tools = self._create_tools()\n",
    "        self.agent = self._create_agent()\n",
    "\n",
    "    def _create_tools(self) -> List[Tool]:\n",
    "        def search_documentation(query: str) -> str:\n",
    "            try:\n",
    "                results = self.qa_chain({\"query\": query})\n",
    "                answer = results[\"result\"]\n",
    "                sources = [doc.metadata.get(\"source\", \"Unknown\") for doc in results[\"source_documents\"]]\n",
    "                unique_sources = \", \".join(sorted(set(sources)))\n",
    "                return f\"Answer: {answer}\\n\\nSources: {unique_sources}\"\n",
    "            except Exception as e:\n",
    "                return f\"Error searching documentation: {str(e)}\"\n",
    "\n",
    "        def search_web(query: str) -> str:\n",
    "            try:\n",
    "                response = self.tavily_client.search(query=query, max_results=3)\n",
    "                results = []\n",
    "                for result in response.get(\"results\", []):\n",
    "                    results.append(\n",
    "                        f\"Title: {result.get('title','N/A')}\\n\"\n",
    "                        f\"Content: {result.get('content','')}\\n\"\n",
    "                        f\"URL: {result.get('url','')}\"\n",
    "                    )\n",
    "                return \"\\n\\n\".join(results) if results else \"No results found.\"\n",
    "            except Exception as e:\n",
    "                return f\"Error searching web: {str(e)}\"\n",
    "\n",
    "        return [\n",
    "            Tool(\n",
    "                name=\"search_documentation\",\n",
    "                description=\"Search internal company documentation for RFP responses and technical specifications\",\n",
    "                func=search_documentation\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"search_web\",\n",
    "                description=\"Search the web for supplementary information, competitor analysis, or current market trends\",\n",
    "                func=search_web\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def _create_agent(self):\n",
    "        return initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "    def respond_to_rfp(self, question: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            response = self.agent.invoke({\"input\": question})\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": response[\"output\"],\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": f\"Error generating response: {str(e)}\",\n",
    "                \"status\": \"error\"\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New v1\n",
    "class RFPAgent:\n",
    "    def __init__(self, vectorstore, tavily_client=None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.1)  # More reliable model\n",
    "        self.tavily_client = tavily_client or TavilyClient()\n",
    "        \n",
    "        # Create retrieval QA chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def respond_to_rfp(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate RFP response using simple QA chain\"\"\"\n",
    "        try:\n",
    "            # Use invoke for chat models\n",
    "            results = self.qa_chain.invoke({\"query\": question})\n",
    "            \n",
    "            response_text = results['result']\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": response_text,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"response\": f\"Error generating response: {str(e)}\",\n",
    "                \"status\": \"error\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. RAGAS Evaluation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, qa_chain):\n",
    "        \"\"\"\n",
    "        RAG Evaluator for QA systems using RAGAS metrics.\n",
    "        Uses GPT-4 chat model to evaluate:\n",
    "        - Faithfulness\n",
    "        - Answer relevancy\n",
    "        - Context precision\n",
    "        - Context recall\n",
    "        \"\"\"\n",
    "        self.qa_chain = qa_chain\n",
    "        self.metrics = [\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall\n",
    "        ]\n",
    "        # ‚úÖ Use ChatOpenAI for chat completions\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "    def create_evaluation_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Create evaluation dataset with sample RFP questions\"\"\"\n",
    "        evaluation_questions = [\n",
    "            {\n",
    "                \"question\": \"What security features does the Enterprise Data Platform provide?\",\n",
    "                \"ground_truth\": \"The platform provides AES-256 encryption for data at rest, TLS 1.3 for data in transit, LDAP/AD authentication, RBAC authorization, and comprehensive audit logging.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the maximum data volume the platform can handle?\",\n",
    "                \"ground_truth\": \"The platform handles petabytes of data with linear scaling capabilities and supports clusters up to 10,000 nodes.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What cloud platforms does the platform integrate with?\",\n",
    "                \"ground_truth\": \"The platform provides native integration with AWS (S3, EC2, RDS, Redshift), Azure (Blob Storage, Data Factory, Synapse Analytics), and GCP (BigQuery, Cloud Storage, Dataflow).\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What are the minimum system requirements for on-premises deployment?\",\n",
    "                \"ground_truth\": \"Minimum requirements include Intel Xeon or AMD EPYC processors (16 cores), 64GB RAM minimum (256GB recommended), SSD storage with 1TB minimum capacity, and 10 Gigabit Ethernet.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What compliance standards does the platform meet?\",\n",
    "                \"ground_truth\": \"The platform meets SOC 2 Type II, ISO 27001, GDPR compliance standards, with optional HIPAA and PCI DSS add-ons.\"\n",
    "            }\n",
    "        ]\n",
    "        return evaluation_questions\n",
    "\n",
    "    def evaluate_system(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the RAG system using RAGAS metrics\"\"\"\n",
    "        try:\n",
    "            eval_dataset = self.create_evaluation_dataset()\n",
    "            records = []\n",
    "\n",
    "            for item in eval_dataset:\n",
    "                result = self.qa_chain({\"query\": item[\"question\"]})\n",
    "                records.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"answer\": result[\"result\"],\n",
    "                    \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
    "                    \"ground_truth\": item[\"ground_truth\"]\n",
    "                })\n",
    "\n",
    "            # ‚úÖ Convert Python list to Hugging Face Dataset\n",
    "            hf_dataset = Dataset.from_list(records)\n",
    "\n",
    "            # ‚úÖ Run RAGAS evaluation\n",
    "            results = evaluate(\n",
    "                dataset=hf_dataset,\n",
    "                metrics=self.metrics,\n",
    "                llm=self.llm\n",
    "            )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI  # ‚úÖ Use ChatOpenAI instead of OpenAI\n",
    "\n",
    "# ‚úÖ Initialize ChatOpenAI properly\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "# ‚úÖ Build QA chain with chat model\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fa7aafadcc4961b97fbc0e08df746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/powertothefuture/.local/share/uv/python/cpython-3.13.0-macos-aarch64-none/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/tqdm/_monitor.py\"\u001b[0m, line \u001b[35m84\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31minstance.refresh\u001b[0m\u001b[1;31m(nolock=True)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/tqdm/std.py\"\u001b[0m, line \u001b[35m1347\u001b[0m, in \u001b[35mrefresh\u001b[0m\n",
      "    \u001b[31mself.display\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/tqdm/notebook.py\"\u001b[0m, line \u001b[35m171\u001b[0m, in \u001b[35mdisplay\u001b[0m\n",
      "    \u001b[1;31mrtext.value\u001b[0m = right\n",
      "    \u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/traitlets/traitlets.py\"\u001b[0m, line \u001b[35m716\u001b[0m, in \u001b[35m__set__\u001b[0m\n",
      "    \u001b[31mself.set\u001b[0m\u001b[1;31m(obj, value)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/traitlets/traitlets.py\"\u001b[0m, line \u001b[35m706\u001b[0m, in \u001b[35mset\u001b[0m\n",
      "    \u001b[31mobj._notify_trait\u001b[0m\u001b[1;31m(self.name, old_value, new_value)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/traitlets/traitlets.py\"\u001b[0m, line \u001b[35m1513\u001b[0m, in \u001b[35m_notify_trait\u001b[0m\n",
      "    \u001b[31mself.notify_change\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mBunch(\u001b[0m\n",
      "        \u001b[1;31m^^^^^^\u001b[0m\n",
      "    ...<5 lines>...\n",
      "        \u001b[1;31m)\u001b[0m\n",
      "        \u001b[1;31m^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/ipywidgets/widgets/widget.py\"\u001b[0m, line \u001b[35m700\u001b[0m, in \u001b[35mnotify_change\u001b[0m\n",
      "    \u001b[31mself.send_state\u001b[0m\u001b[1;31m(key=name)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/ipywidgets/widgets/widget.py\"\u001b[0m, line \u001b[35m586\u001b[0m, in \u001b[35msend_state\u001b[0m\n",
      "    \u001b[31mself._send\u001b[0m\u001b[1;31m(msg, buffers=buffers)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/ipywidgets/widgets/widget.py\"\u001b[0m, line \u001b[35m825\u001b[0m, in \u001b[35m_send\u001b[0m\n",
      "    \u001b[31mself.comm.send\u001b[0m\u001b[1;31m(data=msg, buffers=buffers)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/comm/base_comm.py\"\u001b[0m, line \u001b[35m144\u001b[0m, in \u001b[35msend\u001b[0m\n",
      "    \u001b[31mself.publish_msg\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31m\"comm_msg\",\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^\u001b[0m\n",
      "    ...<2 lines>...\n",
      "        \u001b[1;31mbuffers=buffers,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/ipykernel/comm/comm.py\"\u001b[0m, line \u001b[35m42\u001b[0m, in \u001b[35mpublish_msg\u001b[0m\n",
      "    parent=\u001b[31mself.kernel.get_parent\u001b[0m\u001b[1;31m()\u001b[0m,\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/.venv/lib/python3.13/site-packages/ipykernel/kernelbase.py\"\u001b[0m, line \u001b[35m797\u001b[0m, in \u001b[35mget_parent\u001b[0m\n",
      "    return \u001b[31mself._shell_parent.get\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[1;35mLookupError\u001b[0m: \u001b[35m<ContextVar name='shell_parent' at 0x105bad300>\u001b[0m\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[14]: TimeoutError()\n",
      "Exception raised in Job[16]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n",
      "Exception raised in Job[18]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation results: {'faithfulness': nan, 'answer_relevancy': 0.9708, 'context_precision': 0.7917, 'context_recall': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "evaluator = RAGEvaluator(qa_chain)\n",
    "scores = evaluator.evaluate_system()\n",
    "print(\"RAGAS evaluation results:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, answer_similarity\n",
    "\n",
    "from ragas import evaluate\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qa_chain,\n",
    "        model_name: str = \"gpt-4\",\n",
    "        retriever_type: str = \"qdrant\",\n",
    "        chunk_size: int = 500\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RAG Evaluator for QA systems using RAGAS metrics with experiment tracking.\n",
    "\n",
    "        Args:\n",
    "            qa_chain: The RetrievalQA chain or similar\n",
    "            model_name: The LLM used for generation and evaluation\n",
    "            retriever_type: Type of retriever or vector store (e.g., qdrant, faiss)\n",
    "            chunk_size: Size of text chunks used in preprocessing\n",
    "        \"\"\"\n",
    "        self.qa_chain = qa_chain\n",
    "        self.model_name = model_name\n",
    "        self.retriever_type = retriever_type\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "        self.metrics = [\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall\n",
    "        ]\n",
    "\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "\n",
    "    def create_evaluation_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Static evaluation dataset with sample RFP questions.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"question\": \"What security features does the Enterprise Data Platform provide?\",\n",
    "                \"ground_truth\": \"The platform provides AES-256 encryption for data at rest, TLS 1.3 for data in transit, LDAP/AD authentication, RBAC authorization, and comprehensive audit logging.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the maximum data volume the platform can handle?\",\n",
    "                \"ground_truth\": \"The platform handles petabytes of data with linear scaling capabilities and supports clusters up to 10,000 nodes.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What cloud platforms does the platform integrate with?\",\n",
    "                \"ground_truth\": \"The platform provides native integration with AWS (S3, EC2, RDS, Redshift), Azure (Blob Storage, Data Factory, Synapse Analytics), and GCP (BigQuery, Cloud Storage, Dataflow).\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What are the minimum system requirements for on-premises deployment?\",\n",
    "                \"ground_truth\": \"Minimum requirements include Intel Xeon or AMD EPYC processors (16 cores), 64GB RAM minimum (256GB recommended), SSD storage with 1TB minimum capacity, and 10 Gigabit Ethernet.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What compliance standards does the platform meet?\",\n",
    "                \"ground_truth\": \"The platform meets SOC 2 Type II, ISO 27001, GDPR compliance standards, with optional HIPAA and PCI DSS add-ons.\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def evaluate_system(self) -> Dict:\n",
    "        \"\"\"Run RAGAS evaluation and return metrics and detailed results.\"\"\"\n",
    "        try:\n",
    "            eval_dataset = self.create_evaluation_dataset()\n",
    "            records = []\n",
    "\n",
    "            for item in eval_dataset:\n",
    "                result = self.qa_chain({\"query\": item[\"question\"]})\n",
    "                records.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"answer\": result[\"result\"],\n",
    "                    \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
    "                    \"ground_truth\": item[\"ground_truth\"]\n",
    "                })\n",
    "\n",
    "            # Convert to HF dataset\n",
    "            hf_dataset = Dataset.from_list(records)\n",
    "\n",
    "            # Run RAGAS\n",
    "            scores = evaluate(\n",
    "                dataset=hf_dataset,\n",
    "                metrics=self.metrics,\n",
    "                llm=self.llm\n",
    "            )\n",
    "\n",
    "            return {\"scores\": scores, \"records\": records}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def save_results_to_csv(self, evaluation, scores_file=\"ragas_scores.csv\", details_file=\"ragas_details.csv\"):\n",
    "    import os\n",
    "    import csv\n",
    "    from datetime import datetime\n",
    "\n",
    "    if \"scores\" in evaluation:\n",
    "        # Convert scores to dict if needed\n",
    "        if not isinstance(evaluation[\"scores\"], dict):\n",
    "            scores = evaluation[\"scores\"].to_dict()\n",
    "        else:\n",
    "            scores = evaluation[\"scores\"]\n",
    "\n",
    "        scores_fieldnames = [\n",
    "            \"timestamp\", \"model_name\", \"retriever_type\", \"chunk_size\"\n",
    "        ] + list(scores.keys())\n",
    "\n",
    "        file_exists = os.path.isfile(scores_file)\n",
    "\n",
    "        with open(scores_file, mode='a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=scores_fieldnames)\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            row = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"model_name\": self.model_name,\n",
    "                \"retriever_type\": self.retriever_type,\n",
    "                \"chunk_size\": self.chunk_size,\n",
    "                **scores\n",
    "            }\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # You can similarly handle 'details' if you save detailed results\n",
    "def save_results_to_csv(\n",
    "        self,\n",
    "        evaluation: Dict,\n",
    "        scores_file: str = \"ragas_scores.csv\",\n",
    "        details_file: str = \"ragas_details.csv\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save RAGAS evaluation results to CSV with metadata (model, retriever, chunk size).\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # 1Ô∏è‚É£ Save summary metric scores\n",
    "        if \"scores\" in evaluation:\n",
    "            scores = evaluation[\"scores\"]\n",
    "            scores_fieldnames = [\n",
    "                \"timestamp\", \"model_name\", \"retriever_type\", \"chunk_size\"\n",
    "            ] + list(scores.keys())\n",
    "            file_exists = os.path.isfile(scores_file)\n",
    "\n",
    "            with open(scores_file, mode='a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=scores_fieldnames)\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "\n",
    "                row = {\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"retriever_type\": self.retriever_type,\n",
    "                    \"chunk_size\": self.chunk_size\n",
    "                }\n",
    "                row.update(scores)\n",
    "                writer.writerow(row)\n",
    "\n",
    "            print(f\"‚úÖ RAGAS metric scores saved to: {scores_file}\")\n",
    "\n",
    "        # 2Ô∏è‚É£ Save detailed Q&A logs\n",
    "        if \"records\" in evaluation:\n",
    "            records = evaluation[\"records\"]\n",
    "            details_fieldnames = [\n",
    "                \"timestamp\", \"model_name\", \"retriever_type\", \"chunk_size\",\n",
    "                \"question\", \"answer\", \"ground_truth\", \"contexts\"\n",
    "            ]\n",
    "            file_exists = os.path.isfile(details_file)\n",
    "\n",
    "            with open(details_file, mode='a', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=details_fieldnames)\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "\n",
    "                for record in records:\n",
    "                    writer.writerow({\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"model_name\": self.model_name,\n",
    "                        \"retriever_type\": self.retriever_type,\n",
    "                        \"chunk_size\": self.chunk_size,\n",
    "                        \"question\": record[\"question\"],\n",
    "                        \"answer\": record[\"answer\"],\n",
    "                        \"ground_truth\": record[\"ground_truth\"],\n",
    "                        \"contexts\": \" | \".join(record[\"contexts\"])\n",
    "                    })\n",
    "\n",
    "            print(f\"‚úÖ Detailed evaluation logs saved to: {details_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1f162649ad44d89e04557ff934e8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[16]: TimeoutError()\n",
      "Exception raised in Job[17]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RAGAS Metrics: {'faithfulness': nan, 'answer_relevancy': 0.9619, 'context_precision': 0.8889, 'context_recall': 0.9375}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EvaluationResult' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä RAGAS Metrics:\u001b[39m\u001b[33m\"\u001b[39m, evaluation[\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Save experiment results with metadata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_results_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mRAGEvaluator.save_results_to_csv\u001b[39m\u001b[34m(self, evaluation, scores_file, details_file)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m evaluation:\n\u001b[32m    110\u001b[39m     scores = evaluation[\u001b[33m\"\u001b[39m\u001b[33mscores\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    111\u001b[39m     scores_fieldnames = [\n\u001b[32m    112\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mretriever_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     ] + \u001b[38;5;28mlist\u001b[39m(\u001b[43mscores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m())\n\u001b[32m    114\u001b[39m     file_exists = os.path.isfile(scores_file)\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(scores_file, mode=\u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m, newline=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n",
      "\u001b[31mAttributeError\u001b[39m: 'EvaluationResult' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Pass the same qa_chain you used in RFPAgent\n",
    "evaluator = RAGEvaluator(\n",
    "    qa_chain,\n",
    "    model_name=\"gpt-4\",\n",
    "    retriever_type=\"qdrant\",\n",
    "    chunk_size=500\n",
    ")\n",
    "\n",
    "# Run RAGAS evaluation\n",
    "evaluation = evaluator.evaluate_system()\n",
    "print(\"üìä RAGAS Metrics:\", evaluation[\"scores\"])\n",
    "\n",
    "# Save experiment results with metadata\n",
    "evaluator.save_results_to_csv(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        self.llm = OpenAI(model_name=\"gpt-4\")\n",
    "    \n",
    "    def create_evaluation_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Create evaluation dataset with sample RFP questions\"\"\"\n",
    "        evaluation_questions = [\n",
    "            {\n",
    "                \"question\": \"What security features does the Enterprise Data Platform provide?\",\n",
    "                \"ground_truth\": \"The platform provides AES-256 encryption for data at rest, TLS 1.3 for data in transit, LDAP/AD authentication, RBAC authorization, and comprehensive audit logging.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the maximum data volume the platform can handle?\",\n",
    "                \"ground_truth\": \"The platform handles petabytes of data with linear scaling capabilities and supports clusters up to 10,000 nodes.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What cloud platforms does the platform integrate with?\",\n",
    "                \"ground_truth\": \"The platform provides native integration with AWS (S3, EC2, RDS, Redshift), Azure (Blob Storage, Data Factory, Synapse Analytics), and GCP (BigQuery, Cloud Storage, Dataflow).\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What are the minimum system requirements for on-premises deployment?\",\n",
    "                \"ground_truth\": \"Minimum requirements include Intel Xeon or AMD EPYC processors (16 cores), 64GB RAM minimum (256GB recommended), SSD storage with 1TB minimum capacity, and 10 Gigabit Ethernet.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What compliance standards does the platform meet?\",\n",
    "                \"ground_truth\": \"The platform meets SOC 2 Type II, ISO 27001, GDPR compliance standards, with optional HIPAA and PCI DSS add-ons.\"\n",
    "            }\n",
    "        ]\n",
    "        return evaluation_questions\n",
    "    \n",
    "    def evaluate_system(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the RAG system using RAGAS metrics\"\"\"\n",
    "        try:\n",
    "            # Create evaluation dataset\n",
    "            eval_dataset = self.create_evaluation_dataset()\n",
    "            \n",
    "            # Convert to RAGAS format\n",
    "            dataset = []\n",
    "            for item in eval_dataset:\n",
    "                # Get response from QA chain\n",
    "                result = self.qa_chain({\"query\": item[\"question\"]})\n",
    "                \n",
    "                dataset.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"answer\": result[\"result\"],\n",
    "                    \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
    "                    \"ground_truth\": item[\"ground_truth\"]\n",
    "                })\n",
    "            \n",
    "            # Evaluate using RAGAS\n",
    "            results = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=self.metrics,\n",
    "                llm=self.llm\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new v1\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")  # Use ChatOpenAI for evaluator too\n",
    "    \n",
    "    def create_evaluation_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Create evaluation dataset with sample RFP questions\"\"\"\n",
    "        evaluation_questions = [\n",
    "            {\n",
    "                \"question\": \"What security features does the Enterprise Data Platform provide?\",\n",
    "                \"ground_truth\": \"The platform provides AES-256 encryption for data at rest, TLS 1.3 for data in transit, LDAP/AD authentication, RBAC authorization, and comprehensive audit logging.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the maximum data volume the platform can handle?\",\n",
    "                \"ground_truth\": \"The platform handles petabytes of data with linear scaling capabilities and supports clusters up to 10,000 nodes.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What cloud platforms does the platform integrate with?\",\n",
    "                \"ground_truth\": \"The platform provides native integration with AWS (S3, EC2, RDS, Redshift), Azure (Blob Storage, Data Factory, Synapse Analytics), and GCP (BigQuery, Cloud Storage, Dataflow).\"\n",
    "            }\n",
    "        ]\n",
    "        return evaluation_questions\n",
    "    \n",
    "    def evaluate_system(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the RAG system using RAGAS metrics\"\"\"\n",
    "        try:\n",
    "            # Create evaluation dataset\n",
    "            eval_dataset = self.create_evaluation_dataset()\n",
    "            \n",
    "            # Convert to RAGAS format\n",
    "            dataset = []\n",
    "            for item in eval_dataset:\n",
    "                # Get response from QA chain\n",
    "                result = self.qa_chain.invoke({\"query\": item[\"question\"]})\n",
    "                \n",
    "                dataset.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"answer\": result[\"result\"],\n",
    "                    \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
    "                    \"ground_truth\": item[\"ground_truth\"]\n",
    "                })\n",
    "            \n",
    "            # Evaluate using RAGAS\n",
    "            results = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=self.metrics,\n",
    "                llm=self.llm\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new v2\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from datasets import Dataset  # Add this import\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, qa_chain):\n",
    "        self.qa_chain = qa_chain\n",
    "        self.metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "        self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "    \n",
    "    def create_evaluation_dataset(self) -> List[Dict]:\n",
    "        \"\"\"Create evaluation dataset with sample RFP questions\"\"\"\n",
    "        evaluation_questions = [\n",
    "            {\n",
    "                \"question\": \"What security features does the Enterprise Data Platform provide?\",\n",
    "                \"ground_truth\": \"The platform provides AES-256 encryption for data at rest, TLS 1.3 for data in transit, LDAP/AD authentication, RBAC authorization, and comprehensive audit logging.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is the maximum data volume the platform can handle?\",\n",
    "                \"ground_truth\": \"The platform handles petabytes of data with linear scaling capabilities and supports clusters up to 10,000 nodes.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What cloud platforms does the platform integrate with?\",\n",
    "                \"ground_truth\": \"The platform provides native integration with AWS (S3, EC2, RDS, Redshift), Azure (Blob Storage, Data Factory, Synapse Analytics), and GCP (BigQuery, Cloud Storage, Dataflow).\"\n",
    "            }\n",
    "        ]\n",
    "        return evaluation_questions\n",
    "    \n",
    "    def evaluate_system(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the RAG system using RAGAS metrics\"\"\"\n",
    "        try:\n",
    "            # Create evaluation dataset\n",
    "            eval_dataset = self.create_evaluation_dataset()\n",
    "            \n",
    "            # Convert to RAGAS format\n",
    "            dataset_list = []\n",
    "            for item in eval_dataset:\n",
    "                # Get response from QA chain\n",
    "                result = self.qa_chain.invoke({\"query\": item[\"question\"]})\n",
    "                \n",
    "                dataset_list.append({\n",
    "                    \"question\": item[\"question\"],\n",
    "                    \"answer\": result[\"result\"],\n",
    "                    \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
    "                    \"ground_truth\": item[\"ground_truth\"]\n",
    "                })\n",
    "            \n",
    "            # Convert to HuggingFace Dataset format\n",
    "            dataset = Dataset.from_list(dataset_list)\n",
    "            \n",
    "            # Evaluate using RAGAS\n",
    "            results = evaluate(\n",
    "                dataset=dataset,\n",
    "                metrics=self.metrics,\n",
    "                llm=self.llm\n",
    "            )\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation error: {str(e)}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Main Application Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFPAssistantApp:\n",
    "    def __init__(self, data_path: str = \"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data\"):\n",
    "        self.data_path = data_path\n",
    "        self.processor = DocumentProcessor(data_path)\n",
    "        self.vector_manager = VectorStoreManager()\n",
    "        self.agent = None\n",
    "        self.evaluator = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the RFP Assistant system\"\"\"\n",
    "        print(\"üöÄ Initializing RFP Assistant...\")\n",
    "        \n",
    "        # Load and chunk documents\n",
    "        documents = self.processor.load_documents()\n",
    "        chunks = self.processor.chunk_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = self.vector_manager.create_vectorstore(chunks)\n",
    "        \n",
    "        # Initialize agent (using RFPAgent)\n",
    "        self.agent = RFPAgent(vectorstore)\n",
    "        \n",
    "        # Initialize evaluator\n",
    "        self.evaluator = RAGEvaluator(self.agent.qa_chain)\n",
    "        \n",
    "        print(\"‚úÖ RFP Assistant initialized successfully!\")\n",
    "    \n",
    "    def ask_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question to the RFP Assistant\"\"\"\n",
    "        if not self.agent:\n",
    "            return {\"error\": \"System not initialized. Call initialize() first.\"}\n",
    "        \n",
    "        return self.agent.respond_to_rfp(question)\n",
    "    \n",
    "    def evaluate_performance(self) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the system performance\"\"\"\n",
    "        if not self.evaluator:\n",
    "            return {\"error\": \"Evaluator not initialized. Call initialize() first.\"}\n",
    "        \n",
    "        return self.evaluator.evaluate_system()\n",
    "    \n",
    "    def demo_questions(self):\n",
    "        \"\"\"Run demo questions to showcase the system\"\"\"\n",
    "        demo_questions = [\n",
    "            \"What security features does the Enterprise Data Platform provide?\",\n",
    "            \"What is the platform's availability and disaster recovery capabilities?\",\n",
    "            \"What cloud platforms does the platform integrate with?\",\n",
    "            \"What are the pricing and licensing options?\",\n",
    "            \"What support options are available for enterprise customers?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüéØ Running Demo Questions...\")\n",
    "        for i, question in enumerate(demo_questions, 1):\n",
    "            print(f\"\\n--- Question {i} ---\")\n",
    "            print(f\"Q: {question}\")\n",
    "            \n",
    "            response = self.ask_question(question)\n",
    "            print(f\"A: {response['response']}\")\n",
    "            print(f\"Status: {response['status']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RFP Assistant...\n",
      "Loaded 3 documents\n",
      "Created 27 chunks from 3 documents\n",
      "‚úÖ Created collection: rfp_docs\n",
      "‚ùå Failed to add documents: Client.__init__() got an unexpected keyword argument 'client'\n",
      "‚ùå Error during quick test: Client.__init__() got an unexpected keyword argument 'client'\n",
      "\n",
      "‚ùå Quick test failed. Check the error messages above.\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify everything works\n",
    "def quick_test():\n",
    "    \"\"\"Quick test of the RFP Assistant\"\"\"\n",
    "    try:\n",
    "        # Create the application\n",
    "        app = RFPAssistantApp()\n",
    "        \n",
    "        # Initialize the system\n",
    "        app.initialize()\n",
    "        \n",
    "        # Test a simple question\n",
    "        test_question = \"What security features does the Enterprise Data Platform provide?\"\n",
    "        print(f\"\\nüîç Testing question: {test_question}\")\n",
    "        \n",
    "        response = app.ask_question(test_question)\n",
    "        print(f\"Response: {response['response']}\")\n",
    "        print(f\"Status: {response['status']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during quick test: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run the quick test\n",
    "if __name__ == \"__main__\":\n",
    "    success = quick_test()\n",
    "    if success:\n",
    "        print(\"\\n‚úÖ Quick test completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Quick test failed. Check the error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing RFP Assistant...\n",
      "Loaded 3 documents\n",
      "Created 27 chunks from 3 documents\n",
      "Created Chroma vector store with 27 documents\n",
      "‚úÖ RFP Assistant initialized successfully!\n",
      "\n",
      "üéØ Running Demo Questions...\n",
      "\n",
      "--- Question 1 ---\n",
      "Q: What security features does the Enterprise Data Platform provide?\n",
      "A: The Enterprise Data Platform provides several security features:\n",
      "\n",
      "- Data at Rest: All data is encrypted using AES-256 encryption with automatic key rotation.\n",
      "- Data in Transit: It uses TLS 1.3 encryption for all network communications.\n",
      "- Key Management: The platform integrates with enterprise key management systems such as AWS KMS, Azure Key Vault, and HashiCorp Vault.\n",
      "- Compliance: The platform meets SOC 2 Type II, ISO 27001, and GDPR requirements.\n",
      "\n",
      "Additional Web Information:\n",
      "- Essential Features of an Enterprise Data Platform for Optimized ...: Security and privacy controls are essential, encompassing fine-grained access controls, data encryption, anonymization and pseudonymization...\n",
      "- Enterprise Data Security: An Easy Guide 101 - SentinelOne: Enterprise data security can be defined as the processes, standards, and procedures that organizations put in place to ensure that their data is secure from unauthorized access as well as from threats...\n",
      "\n",
      "Status: success\n",
      "\n",
      "--- Question 2 ---\n",
      "Q: What is the platform's availability and disaster recovery capabilities?\n",
      "A: The platform ensures high availability and disaster recovery capabilities through several methods. It guarantees a 99.9% SLA, which means there is automatic failover for uptime. It also uses multi-region replication, which allows for cross-region data replication in case of a disaster. The platform's Recovery Time Objective (RTO) is under one hour. Additionally, it uses automated backups for incremental backups with point-in-time recovery. Lastly, it employs active-active clustering to ensure there is no single point of failure.\n",
      "\n",
      "Additional Web Information:\n",
      "- Cloud Platform Enterprise availability and disaster recovery - Acquia: The Cloud Platform edge layer handles all public traffic for Drupal applications, with Platform CDN available to eligible Cloud Platform Enterprise subscribers as a global option for cached content de...\n",
      "- Business continuity and disaster recovery for Dynamics 365 SaaS ...: Microsoft provides business continuity and disaster recovery capabilities to all production type environments in Dynamics 365 and Power Platform software as a service (SAAS) applications. Self-service...\n",
      "\n",
      "Status: success\n",
      "\n",
      "--- Question 3 ---\n",
      "Q: What cloud platforms does the platform integrate with?\n",
      "A: The platform integrates with the following cloud platforms:\n",
      "\n",
      "- **AWS**: S3, EC2, RDS, Redshift, Lambda, and other AWS services\n",
      "- **Azure**: Blob Storage, Data Factory, Synapse Analytics, and Azure services\n",
      "- **Google Cloud**: BigQuery, Cloud Storage, Dataflow, and GCP services\n",
      "- **Multi-cloud**: Hybrid and multi-cloud deployment support\n",
      "\n",
      "Additional Web Information:\n",
      "- 19 Best Cloud Integration Platforms Reviewed In 2025 - The CTO Club: I‚Äôve reviewed and evaluated the most popular cloud integration platforms and shortlisted the best ones to improve data connectivity and enhance workflow efficiency. | 1 | Skyvia | Best for cloud data ...\n",
      "- Top 10: Cloud Platforms | Technology Magazine: # Top 10: Cloud Platforms From Salesforce to Snowflake and Cloudflare to CoreWeave, Technology Magazine runs through the 10 leading cloud platforms Salesforce‚Äôs dominance in the Software-as-a-Service ...\n",
      "\n",
      "Status: success\n",
      "\n",
      "--- Question 4 ---\n",
      "Q: What are the pricing and licensing options?\n",
      "A: The platform offers flexible licensing options:\n",
      "\n",
      "- **Per-core Licensing**: Based on CPU cores in the cluster for compute-intensive workloads\n",
      "- **Data Volume**: Tiered pricing based on data volume processed for storage-focused deployments\n",
      "- **User-based**: Per-user licensing for analytics and reporting features\n",
      "- **Hybrid Models**: Combination of licensing models to optimize costs\n",
      "\n",
      "Additional Web Information:\n",
      "- Describe Microsoft 365 pricing, licensing, and support - Training: MS-900 Introduction to Microsoft 365: Describe Microsoft 365 pricing, licensing, and support - Training | Microsoft Learn MS-900 Introduction to Microsoft 365: Describe Microsoft 365 pricing, licensin...\n",
      "- Licensing vs. Pricing Model: What is the difference and why it matters: Separating the licensing model from the pricing model allows software companies to: Essentially, all components of the licensing model, such as devices, users, features or limitations can be priced se...\n",
      "\n",
      "Status: success\n",
      "\n",
      "--- Question 5 ---\n",
      "Q: What support options are available for enterprise customers?\n",
      "A: Enterprise customers have a custom Service Level Agreement (SLA) with a dedicated support team.\n",
      "\n",
      "Additional Web Information:\n",
      "- Microsoft 365 support plans for business and enterprise: # Microsoft 365 support plans for business and enterprise * Support for all Microsoft products | Case management/tooling | Case management/tooling  Microsoft 365 admin center and mobile app, web, and ...\n",
      "- How do you manage your Enterprise customers? : r/CustomerSuccess: They also usually require priority support, features that are customized as possible, and longer-term contracts. Also the sales cycle for...\n",
      "\n",
      "Status: success\n",
      "\n",
      "üìä Evaluating System Performance...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85788be0dd624203a4e7d5cae20e8b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[11]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[9]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[7]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[2]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[4]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[1]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[5]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[6]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[10]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[8]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[3]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n",
      "ERROR:ragas.executor:Exception raised in Job[0]: NotFoundError(Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'faithfulness': nan, 'answer_relevancy': nan, 'context_precision': nan, 'context_recall': nan}\n",
      "\n",
      "üí¨ Interactive Mode - Ask your RFP questions:\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run the RFP Assistant\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the application\n",
    "    app = RFPAssistantApp()\n",
    "    \n",
    "    # Initialize the system\n",
    "    app.initialize()\n",
    "    \n",
    "    # Run demo questions\n",
    "    app.demo_questions()\n",
    "    \n",
    "    # Evaluate system performance\n",
    "    print(\"\\nüìä Evaluating System Performance...\")\n",
    "    evaluation_results = app.evaluate_performance()\n",
    "    print(f\"Evaluation Results: {evaluation_results}\")\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\nüí¨ Interactive Mode - Ask your RFP questions:\")\n",
    "    while True:\n",
    "        question = input(\"\\nEnter your RFP question (or 'quit' to exit): \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response = app.ask_question(question)\n",
    "        print(f\"\\nResponse: {response['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new \n",
    "class RFPAssistantApp:\n",
    "    def __init__(self, data_path: str = \"/Users/powertothefuture/Documents/aimakerspace/AIE8-cert-challenge/data\"):\n",
    "        self.data_path = data_path\n",
    "        self.processor = DocumentProcessor(data_path)\n",
    "        self.vector_manager = VectorStoreManager()\n",
    "        self.agent = None\n",
    "        self.evaluator = None\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the RFP Assistant system\"\"\"\n",
    "        print(\"üöÄ Initializing RFP Assistant...\")\n",
    "        \n",
    "        # Load and chunk documents\n",
    "        documents = self.processor.load_documents()\n",
    "        chunks = self.processor.chunk_documents(documents)\n",
    "        \n",
    "        # Create vector store\n",
    "        vectorstore = self.vector_manager.create_vectorstore(chunks)\n",
    "        \n",
    "        # Initialize agent\n",
    "        self.agent = RFPAgent(vectorstore)\n",
    "        \n",
    "        # Initialize evaluator (using simple evaluator)\n",
    "        self.evaluator = RAGEvaluator(self.agent.qa_chain)\n",
    "        \n",
    "        print(\"‚úÖ RFP Assistant initialized successfully!\")\n",
    "    \n",
    "    def ask_question(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Ask a question to the RFP Assistant\"\"\"\n",
    "        if not self.agent:\n",
    "            return {\"error\": \"System not initialized. Call initialize() first.\"}\n",
    "        \n",
    "        return self.agent.respond_to_rfp(question)\n",
    "    \n",
    "    def evaluate_performance(self) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate the system performance\"\"\"\n",
    "        if not self.evaluator:\n",
    "            return {\"error\": \"Evaluator not initialized. Call initialize() first.\"}\n",
    "        \n",
    "        return self.evaluator.evaluate_system()\n",
    "    \n",
    "    def demo_questions(self):\n",
    "        \"\"\"Run demo questions to showcase the system\"\"\"\n",
    "        demo_questions = [\n",
    "            \"What security features does the Enterprise Data Platform provide?\",\n",
    "            \"What is the platform's availability and disaster recovery capabilities?\",\n",
    "            \"What cloud platforms does the platform integrate with?\",\n",
    "            \"What are the pricing and licensing options?\",\n",
    "            \"What support options are available for enterprise customers?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüéØ Running Demo Questions...\")\n",
    "        for i, question in enumerate(demo_questions, 1):\n",
    "            print(f\"\\n--- Question {i} ---\")\n",
    "            print(f\"Q: {question}\")\n",
    "            \n",
    "            response = self.ask_question(question)\n",
    "            print(f\"A: {response['response']}\")\n",
    "            print(f\"Status: {response['status']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
